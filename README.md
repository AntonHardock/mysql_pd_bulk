# mysql_pd_bulk (MySQL bulk insert from pandas dataframes)

Inserts a bunch of large static data files into a MySQL database (using MySQL Community Server).
Usecase: Each data file represts one table.

The pandas library already provides the convenient pd.DataFrame.to_sql() method.
It uses SQLAlchemy and allows bulk inserts (full dataframe at once, or chunkwise).
However, when selecting "mysqlconnector" as driver in the SQLAlchemy engine, I found the bulk insert to be slow.
Using the MySQLCursor.executemany() method from mysql.connector directly seems to work a lot faster.
Fixes / other options exist, but I could not find a particular fix for "mysqlconnector".
For example, whole csv files can be uploaded directly (and efficiently) to MySQL Server.
Other file types (like json) would have to be converted first, which I'm trying to avoid.
See here: https://stackoverflow.com/questions/33816918/write-large-pandas-dataframes-to-sql-server-database
Or here: https://github.com/pandas-dev/pandas/issues/15276

Since I wanted to stick with this particular driver (https://dev.mysql.com/doc/connector-python/en/),
here's my attempt at creating a bulk insert function.

For each data file, the following input is needed:
    -Chunked pandas dataframe, generated by one of it's many reader functions
    (each chunk is processed by one insert statement)
    -One dictionary per table, specifying:
        -column names (as keys) and datatypes + column constraints (as values)
        (only columns included in the dictionary are inserted, the rest is ignored)
        -optional foreign key constraints (requires one nested dictionary per constraint)

A demo follows soon.








